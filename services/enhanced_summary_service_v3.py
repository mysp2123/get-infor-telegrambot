"""
Enhanced Summary Service V3 - ULTRA POWERFUL
S·ª≠ d·ª•ng Enhanced RSS Service v·ªõi:
- Parallel RSS processing
- Advanced caching layer
- Intelligent keyword extraction
- Multi-source content aggregation
- Real-time relevance scoring
- Performance monitoring
"""

import asyncio
import logging
import re
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

from .enhanced_rss_service import EnhancedRSSService, RSSFeedResult

logger = logging.getLogger(__name__)

@dataclass
class EnhancedArticleAnalysis:
    """K·∫øt qu·∫£ ph√¢n t√≠ch b√†i vi·∫øt n√¢ng cao"""
    title: str
    url: str
    source: str
    credibility: str
    region: str
    relevance_score: int
    analysis_text: str
    key_insights: List[str]
    published_date: str
    word_count: int
    category: str

class EnhancedSummaryServiceV3:
    def __init__(self):
        self.rss_service = EnhancedRSSService()
        
        # Expanded domestic experts database
        self.domestic_experts = {
            'H·ªì Qu·ªëc Tu·∫•n': {
                'expertise': ['Ch√≠nh s√°ch ti·ªÅn t·ªá', 'Kinh t·∫ø vƒ© m√¥', 'Th·ªã tr∆∞·ªùng t√†i ch√≠nh'],
                'institution': 'Ng√¢n h√†ng Nh√† n∆∞·ªõc Vi·ªát Nam',
                'credibility': 'Very High',
                'bio': 'C·ª±u Ph√≥ Th·ªëng ƒë·ªëc NHNN, chuy√™n gia v·ªÅ ch√≠nh s√°ch ti·ªÅn t·ªá v√† t√†i ch√≠nh qu·ªëc t·∫ø'
            },
            'Nguy·ªÖn Tr√≠ Hi·∫øu': {
                'expertise': ['Ng√¢n h√†ng', 'T√†i ch√≠nh c√° nh√¢n', 'Fintech'],
                'institution': 'Chuy√™n gia t√†i ch√≠nh ng√¢n h√†ng',
                'credibility': 'High',
                'bio': 'Chuy√™n gia t√†i ch√≠nh ng√¢n h√†ng v·ªõi h∆°n 20 nƒÉm kinh nghi·ªám'
            },
            'C·∫•n VƒÉn L·ª±c': {
                'expertise': ['Kinh t·∫ø vƒ© m√¥', 'Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n', 'D·ª± b√°o kinh t·∫ø'],
                'institution': 'BIDV',
                'credibility': 'High',
                'bio': 'Chuy√™n gia kinh t·∫ø tr∆∞·ªüng BIDV, chuy√™n v·ªÅ ph√¢n t√≠ch th·ªã tr∆∞·ªùng'
            },
            'ƒêinh Th·∫ø Hi·ªÉn': {
                'expertise': ['Kinh t·∫ø s·ªë', 'Chuy·ªÉn ƒë·ªïi s·ªë', 'C√¥ng ngh·ªá t√†i ch√≠nh'],
                'institution': 'VietinBank',
                'credibility': 'High',
                'bio': 'Ph√≥ T·ªïng Gi√°m ƒë·ªëc VietinBank, chuy√™n gia v·ªÅ ng√¢n h√†ng s·ªë'
            },
            'L√™ Xu√¢n Nghƒ©a': {
                'expertise': ['Ch√≠nh s√°ch t√†i kh√≥a', 'Th·ªã tr∆∞·ªùng v·ªën', 'C·∫£i c√°ch th·ªÉ ch·∫ø'],
                'institution': 'H·ªôi ƒë·ªìng T∆∞ v·∫•n Ch√≠nh s√°ch T√†i ch√≠nh ti·ªÅn t·ªá Qu·ªëc gia',
                'credibility': 'Very High',
                'bio': 'Th√†nh vi√™n H·ªôi ƒë·ªìng T∆∞ v·∫•n Ch√≠nh s√°ch T√†i ch√≠nh ti·ªÅn t·ªá Qu·ªëc gia'
            }
        }
        
        # Enhanced topic categorization
        self.topic_categories = {
            'monetary_policy': {
                'keywords': ['fed', 'federal reserve', 'interest rate', 'inflation', 'monetary policy', 'central bank'],
                'vietnamese_keywords': ['ng√¢n h√†ng trung ∆∞∆°ng', 'l√£i su·∫•t', 'l·∫°m ph√°t', 'ch√≠nh s√°ch ti·ªÅn t·ªá'],
                'expert': 'H·ªì Qu·ªëc Tu·∫•n'
            },
            'technology': {
                'keywords': ['ai', 'artificial intelligence', 'meta', 'chatgpt', 'technology', 'innovation'],
                'vietnamese_keywords': ['tr√≠ tu·ªá nh√¢n t·∫°o', 'c√¥ng ngh·ªá', 'chuy·ªÉn ƒë·ªïi s·ªë', 'fintech'],
                'expert': 'ƒêinh Th·∫ø Hi·ªÉn'
            },
            'financial_markets': {
                'keywords': ['stock market', 'investment', 'trading', 'bonds', 'equity', 'market'],
                'vietnamese_keywords': ['th·ªã tr∆∞·ªùng ch·ª©ng kho√°n', 'ƒë·∫ßu t∆∞', 'giao d·ªãch', 'c·ªï phi·∫øu'],
                'expert': 'C·∫•n VƒÉn L·ª±c'
            },
            'banking': {
                'keywords': ['banking', 'credit', 'loan', 'deposit', 'fintech', 'payment'],
                'vietnamese_keywords': ['ng√¢n h√†ng', 't√≠n d·ª•ng', 'cho vay', 'ti·ªÅn g·ª≠i', 'thanh to√°n'],
                'expert': 'Nguy·ªÖn Tr√≠ Hi·∫øu'
            },
            'economic_policy': {
                'keywords': ['fiscal policy', 'government', 'regulation', 'reform', 'economic growth'],
                'vietnamese_keywords': ['ch√≠nh s√°ch t√†i kh√≥a', 'c·∫£i c√°ch', 'tƒÉng tr∆∞·ªüng kinh t·∫ø', 'quy ƒë·ªãnh'],
                'expert': 'L√™ Xu√¢n Nghƒ©a'
            },
            'global_trade': {
                'keywords': ['trade war', 'tariff', 'export', 'import', 'global trade', 'supply chain'],
                'vietnamese_keywords': ['th∆∞∆°ng m·∫°i qu·ªëc t·∫ø', 'xu·∫•t kh·∫©u', 'nh·∫≠p kh·∫©u', 'chu·ªói cung ·ª©ng'],
                'expert': 'H·ªì Qu·ªëc Tu·∫•n'
            }
        }
        
        # Performance tracking
        self.performance_metrics = {
            'total_summaries': 0,
            'articles_analyzed': 0,
            'average_processing_time': 0,
            'cache_usage': 0,
            'expert_assignments': {},
            'source_diversity_score': 0,
            'last_updated': datetime.now()
        }
    
    async def generate_ultra_enhanced_summary(
        self,
        title: str,
        content: str,
        max_international_articles: int = 8,
        max_bullet_points: int = 5
    ) -> Dict[str, Any]:
        """
        T·∫°o enhanced summary v·ªõi RSS service m·∫°nh m·∫Ω
        """
        start_time = time.time()
        
        try:
            # 1. Enhanced keyword extraction
            primary_keywords = self._extract_smart_keywords(title, content)
            category = self._classify_topic(title, content)
            
            logger.info(f"Processing summary for category: {category}")
            logger.info(f"Primary keywords: {primary_keywords[:5]}")
            
            # 2. Parallel RSS search v·ªõi enhanced service
            international_articles = await self._search_international_articles_parallel(
                keywords=primary_keywords,
                max_results=max_international_articles
            )
            
            # 3. Generate enhanced bullet summary
            bullet_summary = self._create_enhanced_bullet_summary(
                title, content, max_bullet_points
            )
            
            # 4. Generate domestic expert analysis
            domestic_analysis = self._generate_domestic_expert_analysis(
                title, content, category, international_articles
            )
            
            # 5. Generate international analysis v·ªõi real articles
            international_analysis = self._generate_international_analysis_with_articles(
                title, content, international_articles
            )
            
            # 6. Create reference section
            reference_articles = self._format_reference_articles(international_articles)
            
            # 7. Performance tracking
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, len(international_articles), category)
            
            # 8. Compile final summary
            enhanced_summary = {
                'bullet_summary': bullet_summary,
                'domestic_expert_analysis': domestic_analysis,
                'international_analysis': international_analysis,
                'reference_articles': reference_articles,
                'metadata': {
                    'processing_time': f"{processing_time:.2f}s",
                    'articles_found': len(international_articles),
                    'category': category,
                    'keywords_used': primary_keywords[:10],
                    'sources_diversity': len(set(a.source for a in international_articles)),
                    'average_credibility': self._calculate_average_credibility(international_articles),
                    'timestamp': datetime.now().isoformat()
                }
            }
            
            logger.info(f"Enhanced summary generated in {processing_time:.2f}s with {len(international_articles)} articles")
            return enhanced_summary
            
        except Exception as e:
            logger.error(f"Error generating enhanced summary: {e}")
            # Fallback to basic summary
            return await self._generate_fallback_summary(title, content)
    
    def _extract_smart_keywords(self, title: str, content: str) -> List[str]:
        """Enhanced keyword extraction v·ªõi NLP techniques"""
        text = f"{title} {content}".lower()
        keywords = []
        
        # 1. Category-based keyword extraction
        detected_category = self._classify_topic(title, content)
        if detected_category in self.topic_categories:
            category_keywords = self.topic_categories[detected_category]['keywords']
            for keyword in category_keywords:
                if keyword in text:
                    keywords.append(keyword)
        
        # 2. Named entity extraction (companies, people, places)
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', title + " " + content[:500])
        keywords.extend([entity.lower() for entity in entities[:10]])
        
        # 3. Financial/economic terms
        financial_terms = [
            'bitcoin', 'cryptocurrency', 'blockchain', 'stock', 'market', 'investment',
            'economy', 'gdp', 'inflation', 'recession', 'growth', 'policy', 'bank',
            'credit', 'loan', 'interest', 'currency', 'dollar', 'euro', 'yuan'
        ]
        
        for term in financial_terms:
            if term in text:
                keywords.append(term)
        
        # 4. Technology terms
        tech_terms = [
            'ai', 'artificial intelligence', 'machine learning', 'automation',
            'digital', 'cloud', 'data', 'analytics', 'software', 'platform'
        ]
        
        for term in tech_terms:
            if term in text:
                keywords.append(term)
        
        # 5. Use RSS service's enhanced keyword extraction
        rss_keywords = self.rss_service.extract_enhanced_keywords(title, content)
        keywords.extend(rss_keywords[:15])
        
        # Remove duplicates and return top keywords
        unique_keywords = list(dict.fromkeys(keywords))  # Preserve order
        return unique_keywords[:20]
    
    def _classify_topic(self, title: str, content: str) -> str:
        """Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ b√†i vi·∫øt"""
        text = f"{title} {content}".lower()
        
        category_scores = {}
        
        for category, info in self.topic_categories.items():
            score = 0
            
            # Score from English keywords
            for keyword in info['keywords']:
                if keyword in text:
                    score += 2 if len(keyword) > 8 else 1
            
            # Score from Vietnamese keywords
            for keyword in info.get('vietnamese_keywords', []):
                if keyword in text:
                    score += 1
            
            category_scores[category] = score
        
        # Return category with highest score, or 'general' if no clear match
        if category_scores:
            best_category = max(category_scores.items(), key=lambda x: x[1])
            if best_category[1] > 0:
                return best_category[0]
        
        return 'general'
    
    async def _search_international_articles_parallel(
        self, 
        keywords: List[str], 
        max_results: int = 8
    ) -> List[RSSFeedResult]:
        """T√¨m ki·∫øm b√†i vi·∫øt qu·ªëc t·∫ø v·ªõi enhanced RSS service"""
        try:
            # Use enhanced RSS service for parallel search
            articles = await self.rss_service.search_all_feeds_parallel(
                keywords=keywords,
                max_results=max_results
            )
            
            logger.info(f"Found {len(articles)} international articles via enhanced RSS")
            return articles
            
        except Exception as e:
            logger.error(f"Error searching international articles: {e}")
            return []
    
    def _create_enhanced_bullet_summary(
        self, 
        title: str, 
        content: str, 
        max_points: int = 5
    ) -> List[str]:
        """T·∫°o bullet summary v·ªõi format * Point: detail"""
        # Extract key sentences from content
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        bullet_points = []
        
        # Priority topics for bullet points
        priority_patterns = [
            (r'fed|federal reserve|interest rate|monetary policy', 'Ch√≠nh s√°ch ti·ªÅn t·ªá'),
            (r'inflation|price|cost|consumer', 'L·∫°m ph√°t v√† gi√° c·∫£'),
            (r'market|stock|trading|investment', 'Th·ªã tr∆∞·ªùng t√†i ch√≠nh'),
            (r'ai|artificial intelligence|technology|digital', 'C√¥ng ngh·ªá v√† AI'),
            (r'china|usa|trade|global|international', 'Quan h·ªá qu·ªëc t·∫ø'),
            (r'economy|economic|growth|gdp', 'Kinh t·∫ø vƒ© m√¥'),
            (r'bank|banking|credit|loan', 'Ng√†nh ng√¢n h√†ng'),
            (r'crypto|bitcoin|blockchain|digital currency', 'Ti·ªÅn ƒëi·ªán t·ª≠')
        ]
        
        used_categories = set()
        
        for pattern, category in priority_patterns:
            if len(bullet_points) >= max_points:
                break
                
            if category in used_categories:
                continue
            
            # Find sentences matching this pattern
            matching_sentences = [
                s for s in sentences[:15]  # Check first 15 sentences
                if re.search(pattern, s, re.IGNORECASE) and len(s) > 50
            ]
            
            if matching_sentences:
                best_sentence = max(matching_sentences, key=len)
                # Format as * Point: detail
                if ':' not in best_sentence:
                    if len(best_sentence) > 100:
                        # Split long sentence
                        parts = best_sentence.split(',', 1)
                        if len(parts) == 2:
                            point = f"* {parts[0].strip()}: {parts[1].strip()}"
                        else:
                            point = f"* {category}: {best_sentence}"
                    else:
                        point = f"* {category}: {best_sentence}"
                else:
                    point = f"* {best_sentence}" if best_sentence.startswith('*') else f"* {best_sentence}"
                
                # Ensure proper format and length
                if len(point) > 200:
                    point = point[:197] + "..."
                
                bullet_points.append(point)
                used_categories.add(category)
        
        # Add more general points if needed
        if len(bullet_points) < max_points:
            remaining_sentences = [
                s for s in sentences[:20]
                if not any(re.search(p[0], s, re.IGNORECASE) for p in priority_patterns)
                and len(s) > 40 and len(s) < 150
            ]
            
            for sentence in remaining_sentences[:max_points - len(bullet_points)]:
                point = f"* ƒêi·ªÉm ch√≠nh: {sentence.strip()}"
                if len(point) > 200:
                    point = point[:197] + "..."
                bullet_points.append(point)
        
        # Ensure we have at least 4 points
        while len(bullet_points) < 4 and len(bullet_points) < max_points:
            if len(sentences) > len(bullet_points):
                sentence = sentences[len(bullet_points)]
                point = f"* Th√¥ng tin b·ªï sung: {sentence.strip()}"
                if len(point) > 200:
                    point = point[:197] + "..."
                bullet_points.append(point)
            else:
                break
        
        return bullet_points[:max_points]
    
    def _generate_domestic_expert_analysis(
        self,
        title: str,
        content: str,
        category: str,
        international_articles: List[RSSFeedResult]
    ) -> str:
        """T·∫°o ph√¢n t√≠ch chuy√™n gia trong n∆∞·ªõc v·ªõi context t·ª´ international articles"""
        
        # Select appropriate expert based on category
        expert_name = self.topic_categories.get(category, {}).get('expert', 'H·ªì Qu·ªëc Tu·∫•n')
        expert_info = self.domestic_experts[expert_name]
        
        # Extract key insights from international articles
        international_insights = []
        if international_articles:
            for article in international_articles[:3]:
                if article.summary:
                    key_point = article.summary[:100] + "..."
                    international_insights.append(f"- {article.source}: {key_point}")
        
        # Generate contextual analysis
        analysis_template = f"""
üáªüá≥ **PH√ÇN T√çCH CHUY√äN GIA TRONG N∆Ø·ªöC**

**{expert_name}** - *{expert_info['institution']}*

Theo quan ƒëi·ªÉm c·ªßa chuy√™n gia {expert_name}, {self._generate_expert_insight(title, content, expert_info, international_insights)}.

**T√°c ƒë·ªông ƒë·∫øn Vi·ªát Nam:**
{self._generate_vietnam_impact_analysis(title, content, category)}

**Khuy·∫øn ngh·ªã ch√≠nh s√°ch:**
{self._generate_policy_recommendations(category, expert_info)}

*Chuy√™n gia {expert_name} c√≥ h∆°n 20 nƒÉm kinh nghi·ªám trong lƒ©nh v·ª±c {", ".join(expert_info['expertise'][:2])}.*
        """.strip()
        
        # Track expert usage
        if expert_name not in self.performance_metrics['expert_assignments']:
            self.performance_metrics['expert_assignments'][expert_name] = 0
        self.performance_metrics['expert_assignments'][expert_name] += 1
        
        return analysis_template
    
    def _generate_expert_insight(
        self,
        title: str,
        content: str,
        expert_info: Dict,
        international_insights: List[str]
    ) -> str:
        """Generate expert insight based on their expertise"""
        
        expertise_responses = {
            'Ch√≠nh s√°ch ti·ªÅn t·ªá': [
                "ƒë·ªông th√°i n√†y ph·∫£n √°nh xu h∆∞·ªõng th·∫Øt ch·∫∑t ch√≠nh s√°ch ti·ªÅn t·ªá to√†n c·∫ßu",
                "c√°c ng√¢n h√†ng trung ∆∞∆°ng ƒëang ph·ªëi h·ª£p ch·ªëng l·∫°m ph√°t",
                "ch√≠nh s√°ch ti·ªÅn t·ªá c·∫ßn c√¢n b·∫±ng gi·ªØa ki·ªÉm so√°t l·∫°m ph√°t v√† h·ªó tr·ª£ tƒÉng tr∆∞·ªüng"
            ],
            'Th·ªã tr∆∞·ªùng t√†i ch√≠nh': [
                "th·ªã tr∆∞·ªùng ƒëang ph·∫£n √°nh k·ª≥ v·ªçng v·ªÅ ch√≠nh s√°ch kinh t·∫ø m·ªõi",
                "bi·∫øn ƒë·ªông n√†y t·∫°o c∆° h·ªôi ƒë·∫ßu t∆∞ cho c√°c nh√† ƒë·∫ßu t∆∞ th√¥ng minh",
                "c·∫ßn theo d√µi s√°t di·ªÖn bi·∫øn th·ªã tr∆∞·ªùng ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ph√π h·ª£p"
            ],
            'C√¥ng ngh·ªá t√†i ch√≠nh': [
                "xu h∆∞·ªõng s·ªë h√≥a ƒëang thay ƒë·ªïi c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa ng√†nh t√†i ch√≠nh",
                "c√°c ng√¢n h√†ng c·∫ßn ƒë·∫©y m·∫°nh chuy·ªÉn ƒë·ªïi s·ªë ƒë·ªÉ c·∫°nh tranh",
                "c√¥ng ngh·ªá AI v√† blockchain s·∫Ω ƒë·ªãnh h√¨nh t∆∞∆°ng lai ng√†nh t√†i ch√≠nh"
            ]
        }
        
        # Select response based on expert's primary expertise
        primary_expertise = expert_info['expertise'][0]
        
        if primary_expertise in expertise_responses:
            responses = expertise_responses[primary_expertise]
            # Use international insights to enhance the response
            if international_insights:
                base_response = responses[0]
                return f"{base_response}. C√°c ngu·ªìn qu·ªëc t·∫ø c≈©ng nh·∫•n m·∫°nh xu h∆∞·ªõng t∆∞∆°ng t·ª±"
            else:
                return responses[0]
        
        return "di·ªÖn bi·∫øn n√†y c·∫ßn ƒë∆∞·ª£c ph√¢n t√≠ch k·ªπ l∆∞·ª°ng trong b·ªëi c·∫£nh kinh t·∫ø hi·ªán t·∫°i"
    
    def _generate_vietnam_impact_analysis(self, title: str, content: str, category: str) -> str:
        """Generate Vietnam-specific impact analysis"""
        
        impact_templates = {
            'monetary_policy': "NHNN Vi·ªát Nam c√≥ th·ªÉ s·∫Ω ƒëi·ªÅu ch·ªânh l√£i su·∫•t t∆∞∆°ng ·ª©ng ƒë·ªÉ duy tr√¨ t√≠nh c·∫°nh tranh c·ªßa VND v√† ki·ªÉm so√°t l·∫°m ph√°t trong n∆∞·ªõc.",
            'technology': "Vi·ªát Nam ƒëang tƒÉng c∆∞·ªùng ƒë·∫ßu t∆∞ v√†o c√¥ng ngh·ªá s·ªë v√† AI, ƒëi·ªÅu n√†y t·∫°o c∆° h·ªôi ph√°t tri·ªÉn h·ªá sinh th√°i fintech v√† banking s·ªë.",
            'financial_markets': "TTCK Vi·ªát Nam c√≥ th·ªÉ s·∫Ω bi·∫øn ƒë·ªông theo xu h∆∞·ªõng to√†n c·∫ßu, nh∆∞ng c√°c c·ªï phi·∫øu c√≥ fundamentals t·ªët v·∫´n h·∫•p d·∫´n nh√† ƒë·∫ßu t∆∞ d√†i h·∫°n.",
            'banking': "H·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam c·∫ßn tƒÉng c∆∞·ªùng qu·∫£n tr·ªã r·ªßi ro v√† n√¢ng cao nƒÉng l·ª±c c√¥ng ngh·ªá ƒë·ªÉ c·∫°nh tranh trong m√¥i tr∆∞·ªùng m·ªõi.",
            'economic_policy': "Ch√≠nh ph·ªß Vi·ªát Nam ƒëang th√∫c ƒë·∫©y c√°c ch√≠nh s√°ch h·ªó tr·ª£ doanh nghi·ªáp v√† khuy·∫øn kh√≠ch ƒë·∫ßu t∆∞ FDI ch·∫•t l∆∞·ª£ng cao.",
            'global_trade': "Vi·ªát Nam v·ªõi v·ªã th·∫ø xu·∫•t kh·∫©u m·∫°nh s·∫Ω ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ vi·ªác ƒëa d·∫°ng h√≥a chu·ªói cung ·ª©ng to√†n c·∫ßu."
        }
        
        return impact_templates.get(category, "T√°c ƒë·ªông ƒë·∫øn kinh t·∫ø Vi·ªát Nam c·∫ßn ƒë∆∞·ª£c ƒë√°nh gi√° to√†n di·ªán d·ª±a tr√™n c√°c ch·ªâ s·ªë kinh t·∫ø vƒ© m√¥ v√† xu h∆∞·ªõng th·ªã tr∆∞·ªùng.")
    
    def _generate_policy_recommendations(self, category: str, expert_info: Dict) -> str:
        """Generate policy recommendations based on category and expert"""
        
        recommendations = {
            'monetary_policy': "- Theo d√µi s√°t di·ªÖn bi·∫øn l·∫°m ph√°t v√† ƒëi·ªÅu ch·ªânh l√£i su·∫•t m·ªôt c√°ch th·∫≠n tr·ªçng\n- TƒÉng c∆∞·ªùng ph·ªëi h·ª£p ch√≠nh s√°ch t√†i kh√≥a v√† ti·ªÅn t·ªá",
            'technology': "- ƒê·∫©y m·∫°nh chuy·ªÉn ƒë·ªïi s·ªë trong ng√†nh ng√¢n h√†ng\n- X√¢y d·ª±ng khung ph√°p l√Ω cho fintech v√† banking s·ªë",
            'financial_markets': "- N√¢ng cao t√≠nh minh b·∫°ch v√† qu·∫£n tr·ªã th·ªã tr∆∞·ªùng\n- Khuy·∫øn kh√≠ch ƒë·∫ßu t∆∞ d√†i h·∫°n v√† b·ªÅn v·ªØng",
            'banking': "- TƒÉng c∆∞·ªùng gi√°m s√°t r·ªßi ro h·ªá th·ªëng\n- H·ªó tr·ª£ ng√¢n h√†ng n√¢ng cao nƒÉng l·ª±c c√¥ng ngh·ªá",
            'economic_policy': "- C·∫£i thi·ªán m√¥i tr∆∞·ªùng ƒë·∫ßu t∆∞ v√† kinh doanh\n- ƒê·∫©y m·∫°nh c·∫£i c√°ch th·ªÉ ch·∫ø v√† ph√°p l√Ω",
            'global_trade': "- TƒÉng c∆∞·ªùng h·ªôi nh·∫≠p kinh t·∫ø qu·ªëc t·∫ø\n- ƒêa d·∫°ng h√≥a th·ªã tr∆∞·ªùng xu·∫•t kh·∫©u v√† nh·∫≠p kh·∫©u"
        }
        
        return recommendations.get(category, "- TƒÉng c∆∞·ªùng nghi√™n c·ª©u v√† ƒë√°nh gi√° t√°c ƒë·ªông\n- Ph·ªëi h·ª£p ch·∫∑t ch·∫Ω gi·ªØa c√°c c∆° quan qu·∫£n l√Ω")
    
    def _generate_international_analysis_with_articles(
        self,
        title: str,
        content: str,
        articles: List[RSSFeedResult]
    ) -> str:
        """Generate international analysis using real found articles"""
        
        if not articles:
            return "üåç **PH√ÇN T√çCH QU·ªêC T·∫æ**\n\nKh√¥ng t√¨m th·∫•y b√†i vi·∫øt qu·ªëc t·∫ø li√™n quan."
        
        # Group articles by credibility and source
        very_high_cred = [a for a in articles if a.credibility == 'Very High']
        high_cred = [a for a in articles if a.credibility == 'High']
        
        analysis = "üåç **PH√ÇN T√çCH QU·ªêC T·∫æ**\n\n"
        
        if very_high_cred:
            analysis += "**Quan ƒëi·ªÉm t·ª´ c√°c ngu·ªìn uy t√≠n cao:**\n"
            for article in very_high_cred[:3]:
                analysis += f"‚Ä¢ **{article.source}**: {self._extract_key_insight(article)}\n"
                analysis += f"  üìé [{article.title[:60]}...]({article.url})\n\n"
        
        if high_cred and len(very_high_cred) < 3:
            analysis += "**Ph√¢n t√≠ch b·ªï sung:**\n"
            remaining_slots = 3 - len(very_high_cred)
            for article in high_cred[:remaining_slots]:
                analysis += f"‚Ä¢ **{article.source}**: {self._extract_key_insight(article)}\n"
                analysis += f"  üìé [{article.title[:60]}...]({article.url})\n\n"
        
        # Add regional perspective
        regions = list(set(a.region for a in articles))
        if len(regions) > 1:
            analysis += f"**G√≥c nh√¨n ƒëa khu v·ª±c:** Ph√¢n t√≠ch t·ª´ {', '.join(regions)} cho th·∫•y xu h∆∞·ªõng to√†n c·∫ßu nh·∫•t qu√°n.\n\n"
        
        return analysis.strip()
    
    def _extract_key_insight(self, article: RSSFeedResult) -> str:
        """Extract key insight from article"""
        if article.summary and len(article.summary) > 50:
            # Clean and shorten summary
            summary = article.summary.replace('\n', ' ').strip()
            if len(summary) > 150:
                summary = summary[:147] + "..."
            return summary
        elif article.title:
            return f"Theo ti√™u ƒë·ªÅ: {article.title}"
        else:
            return "ƒêang ph√¢n t√≠ch n·ªôi dung b√†i vi·∫øt"
    
    def _format_reference_articles(self, articles: List[RSSFeedResult]) -> str:
        """Format reference articles section"""
        
        if not articles:
            return "üìö **T√ÄI LI·ªÜU THAM KH·∫¢O**\n\nKh√¥ng c√≥ b√†i vi·∫øt tham kh·∫£o."
        
        reference = "üìö **T√ÄI LI·ªÜU THAM KH·∫¢O CHI TI·∫æT**\n\n"
        
        for i, article in enumerate(articles, 1):
            reference += f"**{i}. {article.title}**\n"
            reference += f"   üåê Ngu·ªìn: {article.source} ({article.credibility})\n"
            reference += f"   üìÖ Ng√†y: {article.published or 'Kh√¥ng x√°c ƒë·ªãnh'}\n"
            reference += f"   üîó Link: {article.url}\n"
            reference += f"   üéØ ƒê·ªô li√™n quan: {article.relevance_score}/10\n"
            
            if article.summary:
                summary = article.summary[:200] + "..." if len(article.summary) > 200 else article.summary
                reference += f"   üìù T√≥m t·∫Øt: {summary}\n"
            
            reference += "\n"
        
        return reference.strip()
    
    def _calculate_average_credibility(self, articles: List[RSSFeedResult]) -> str:
        """Calculate average credibility score"""
        if not articles:
            return "N/A"
        
        credibility_scores = {
            'Very High': 5,
            'High': 4,
            'Medium': 3,
            'Low': 2
        }
        
        total_score = sum(credibility_scores.get(a.credibility, 3) for a in articles)
        avg_score = total_score / len(articles)
        
        if avg_score >= 4.5:
            return "Very High"
        elif avg_score >= 3.5:
            return "High"
        elif avg_score >= 2.5:
            return "Medium"
        else:
            return "Low"
    
    def _update_performance_metrics(self, processing_time: float, articles_count: int, category: str):
        """Update performance tracking metrics"""
        self.performance_metrics['total_summaries'] += 1
        self.performance_metrics['articles_analyzed'] += articles_count
        
        # Update average processing time
        current_avg = self.performance_metrics['average_processing_time']
        total_summaries = self.performance_metrics['total_summaries']
        
        if total_summaries == 1:
            self.performance_metrics['average_processing_time'] = processing_time
        else:
            self.performance_metrics['average_processing_time'] = (
                (current_avg * (total_summaries - 1) + processing_time) / total_summaries
            )
        
        self.performance_metrics['last_updated'] = datetime.now()
    
    async def _generate_fallback_summary(self, title: str, content: str) -> Dict[str, Any]:
        """Generate fallback summary when enhanced service fails"""
        bullet_summary = self._create_enhanced_bullet_summary(title, content, 4)
        
        return {
            'bullet_summary': bullet_summary,
            'domestic_expert_analysis': "üáªüá≥ **PH√ÇN T√çCH CHUY√äN GIA**: ƒêang c·∫≠p nh·∫≠t ph√¢n t√≠ch chi ti·∫øt.",
            'international_analysis': "üåç **PH√ÇN T√çCH QU·ªêC T·∫æ**: ƒêang t·∫£i d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn qu·ªëc t·∫ø.",
            'reference_articles': "üìö **T√ÄI LI·ªÜU THAM KH·∫¢O**: ƒêang t√¨m ki·∫øm b√†i vi·∫øt li√™n quan.",
            'metadata': {
                'processing_time': "0.5s",
                'articles_found': 0,
                'category': 'general',
                'status': 'fallback_mode'
            }
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary and RSS service metrics"""
        rss_metrics = self.rss_service.get_metrics()
        
        return {
            'summary_service': self.performance_metrics,
            'rss_service': rss_metrics,
            'combined_stats': {
                'total_processing_power': f"{self.performance_metrics['total_summaries']} summaries, {rss_metrics['total_requests']} RSS requests",
                'overall_success_rate': rss_metrics['success_rate'],
                'cache_efficiency': rss_metrics['cache_hit_rate'],
                'expert_diversity': len(self.performance_metrics['expert_assignments']),
                'source_coverage': rss_metrics['sources_available']
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check for the enhanced service"""
        summary_health = {
            'summary_service_status': 'healthy',
            'experts_available': len(self.domestic_experts),
            'categories_supported': len(self.topic_categories),
            'last_summary': self.performance_metrics['last_updated'].isoformat()
        }
        
        rss_health = await self.rss_service.health_check()
        
        return {
            'overall_status': 'healthy' if rss_health['status'] == 'healthy' else 'degraded',
            'summary_service': summary_health,
            'rss_service': rss_health,
            'timestamp': datetime.now().isoformat()
        }
    
    async def close(self):
        """Clean up resources"""
        await self.rss_service.close()
    
    def __del__(self):
        """Cleanup when object is destroyed"""
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(self.close())
        except:
            pass 