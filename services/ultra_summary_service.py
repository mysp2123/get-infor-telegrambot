"""
Ultra Summary Service - MAXIMUM POWER VERSION
Sá»­ dá»¥ng Enhanced RSS Service vá»›i táº¥t cáº£ tÃ­nh nÄƒng máº¡nh máº½ nháº¥t
"""

import asyncio
import logging
import re
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from services.enhanced_rss_service import EnhancedRSSService, RSSFeedResult

logger = logging.getLogger(__name__)

class UltraSummaryService:
    def __init__(self):
        self.rss_service = EnhancedRSSService()
        
        # Vietnamese experts database
        self.experts = {
            'Há»“ Quá»‘c Tuáº¥n': {
                'expertise': ['ChÃ­nh sÃ¡ch tiá»n tá»‡', 'Kinh táº¿ vÄ© mÃ´'],
                'institution': 'Cá»±u PhÃ³ Thá»‘ng Ä‘á»‘c NHNN',
                'credibility': 'Very High'
            },
            'Nguyá»…n TrÃ­ Hiáº¿u': {
                'expertise': ['NgÃ¢n hÃ ng', 'TÃ i chÃ­nh'],
                'institution': 'ChuyÃªn gia tÃ i chÃ­nh',
                'credibility': 'High'
            },
            'Cáº¥n VÄƒn Lá»±c': {
                'expertise': ['Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n', 'Kinh táº¿'],
                'institution': 'BIDV',
                'credibility': 'High'
            }
        }
        
    async def generate_ultra_summary(self, title: str, content: str) -> Dict[str, Any]:
        """Táº¡o summary vá»›i sá»©c máº¡nh tá»‘i Ä‘a"""
        start_time = time.time()
        
        try:
            # 1. Extract keywords thÃ´ng minh
            keywords = self._extract_keywords(title, content)
            
            # 2. TÃ¬m kiáº¿m parallel trÃªn táº¥t cáº£ RSS feeds
            print(f"ğŸ” Searching for keywords: {', '.join(keywords[:5])}")
            articles = await self.rss_service.search_all_feeds_parallel(
                keywords=keywords,
                max_results=10
            )
            
            # 3. Táº¡o bullet summary vá»›i format má»›i
            bullets = self._create_bullet_summary(title, content)
            
            # 4. Táº¡o domestic analysis
            domestic = self._create_domestic_analysis(title, content, articles)
            
            # 5. Táº¡o international analysis vá»›i real articles
            international = self._create_international_analysis(articles)
            
            # 6. Táº¡o references
            references = self._create_references(articles)
            
            processing_time = time.time() - start_time
            
            return {
                'bullet_summary': bullets,
                'domestic_analysis': domestic,
                'international_analysis': international,
                'references': references,
                'real_articles': [
                    {
                        'title': article.title,
                        'url': article.url,
                        'source': article.source,
                        'credibility': article.credibility,
                        'relevance_score': article.relevance_score,
                        'summary': article.summary
                    }
                    for article in articles
                ],
                'metadata': {
                    'processing_time': f"{processing_time:.2f}s",
                    'articles_found': len(articles),
                    'keywords_used': keywords[:10],
                    'sources': list(set(a.source for a in articles)),
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"Error: {e}")
            return self._fallback_summary(title, content)
    
    def _extract_keywords(self, title: str, content: str) -> List[str]:
        """Extract smart keywords"""
        text = f"{title} {content}".lower()
        keywords = []
        
        # Important economic/financial terms
        important_terms = [
            'fed', 'federal reserve', 'interest rate', 'inflation', 'monetary policy',
            'artificial intelligence', 'meta', 'chatgpt', 'ai', 'technology',
            'china', 'usa', 'trade war', 'biden', 'trump', 'tariff',
            'bitcoin', 'cryptocurrency', 'stock market', 'investment',
            'economy', 'economic', 'growth', 'recession', 'bank', 'banking'
        ]
        
        for term in important_terms:
            if term in text:
                keywords.append(term)
        
        # Extract company names and proper nouns
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', title + " " + content[:300])
        keywords.extend([e.lower() for e in entities[:10]])
        
        return list(set(keywords))[:15]
    
    def _create_bullet_summary(self, title: str, content: str) -> List[str]:
        """Táº¡o 4-5 bullet points vá»›i format * Point: (no duplicates)"""
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 40]
        
        bullets = []
        used_sentences = set()  # Prevent duplicates
        
        # Key themes to look for
        themes = [
            ('fed|federal reserve|interest rate|monetary policy', 'ChÃ­nh sÃ¡ch tiá»n tá»‡ FED'),
            ('inflation|price|cost|consumer', 'Láº¡m phÃ¡t vÃ  giÃ¡ cáº£'),
            ('ai|artificial intelligence|technology|digital', 'CÃ´ng nghá»‡ AI'),
            ('china|usa|trade|tariff|export|import', 'Quan há»‡ thÆ°Æ¡ng máº¡i'),
            ('market|stock|investment|economy|economic', 'Thá»‹ trÆ°á»ng tÃ i chÃ­nh')
        ]
        
        for pattern, theme in themes:
            if len(bullets) >= 5:
                break
                
            matching = [s for s in sentences[:15] 
                       if re.search(pattern, s, re.IGNORECASE) and 
                       len(s) > 30 and 
                       s.lower() not in used_sentences]
            
            if matching:
                sentence = matching[0]
                # Mark as used to prevent duplicates
                used_sentences.add(sentence.lower())
                
                if ':' not in sentence:
                    bullet = f"* {theme}: {sentence.strip()}"
                else:
                    bullet = f"* {sentence.strip()}"
                    
                # Limit length
                if len(bullet) > 200:
                    bullet = bullet[:197] + "..."
                    
                bullets.append(bullet)
        
        # Add more general points if needed (avoid duplicates)
        remaining_sentences = [s for s in sentences[:20] 
                             if s.lower() not in used_sentences and len(s) > 40]
        
        for sentence in remaining_sentences:
            if len(bullets) >= 5:
                break
                
            bullet = f"* Äiá»ƒm chÃ­nh: {sentence.strip()}"
            if len(bullet) > 200:
                bullet = bullet[:197] + "..."
            bullets.append(bullet)
            used_sentences.add(sentence.lower())
        
        # Ensure minimum 3 bullets with fallback
        if len(bullets) < 3:
            fallback_bullets = [
                f"* Tin tá»©c vá»: {title[:80]}...",
                f"* Ná»™i dung chÃ­nh: {content[:100].strip()}...",
                "* Äang cáº­p nháº­t thÃ´ng tin chi tiáº¿t..."
            ]
            bullets.extend(fallback_bullets[:3-len(bullets)])
                
        return bullets[:5]
    
    def _create_domestic_analysis(self, title: str, content: str, articles: List[RSSFeedResult]) -> str:
        """Táº¡o phÃ¢n tÃ­ch chuyÃªn gia trong nÆ°á»›c"""
        expert = "Há»“ Quá»‘c Tuáº¥n"  # Default expert
        
        analysis = f"""ğŸ‡»ğŸ‡³ **PHÃ‚N TÃCH CHUYÃŠN GIA TRONG NÆ¯á»šC**

**{expert}** - *Cá»±u PhÃ³ Thá»‘ng Ä‘á»‘c NHNN*

Theo Ã´ng {expert}, diá»…n biáº¿n nÃ y pháº£n Ã¡nh xu hÆ°á»›ng chÃ­nh sÃ¡ch tiá»n tá»‡ toÃ n cáº§u vÃ  cÃ³ tÃ¡c Ä‘á»™ng quan trá»ng Ä‘áº¿n ná»n kinh táº¿.

**TÃ¡c Ä‘á»™ng Ä‘áº¿n Viá»‡t Nam:**
- NHNN cÃ³ thá»ƒ Ä‘iá»u chá»‰nh chÃ­nh sÃ¡ch Ä‘á»ƒ phÃ¹ há»£p vá»›i xu hÆ°á»›ng quá»‘c táº¿
- Thá»‹ trÆ°á»ng tÃ i chÃ­nh trong nÆ°á»›c sáº½ cÃ³ nhá»¯ng biáº¿n Ä‘á»™ng tÆ°Æ¡ng á»©ng
- Cáº§n theo dÃµi sÃ¡t diá»…n biáº¿n Ä‘á»ƒ cÃ³ pháº£n á»©ng chÃ­nh sÃ¡ch phÃ¹ há»£p

**Khuyáº¿n nghá»‹:**
- TÄƒng cÆ°á»ng giÃ¡m sÃ¡t thá»‹ trÆ°á»ng tÃ i chÃ­nh
- Phá»‘i há»£p cháº·t cháº½ giá»¯a chÃ­nh sÃ¡ch tÃ i khÃ³a vÃ  tiá»n tá»‡
"""
        
        if articles:
            analysis += f"\n*PhÃ¢n tÃ­ch dá»±a trÃªn {len(articles)} nguá»“n tin quá»‘c táº¿ Ä‘Ã¡ng tin cáº­y.*"
            
        return analysis.strip()
    
    def _create_international_analysis(self, articles: List[RSSFeedResult]) -> str:
        """Táº¡o phÃ¢n tÃ­ch quá»‘c táº¿ tá»« articles thá»±c"""
        if not articles:
            return "ğŸŒ **PHÃ‚N TÃCH QUá»C Táº¾**\n\nÄang cáº­p nháº­t thÃ´ng tin tá»« cÃ¡c nguá»“n quá»‘c táº¿."
        
        analysis = "ğŸŒ **PHÃ‚N TÃCH QUá»C Táº¾**\n\n"
        
        # Group by credibility
        very_high = [a for a in articles if a.credibility == 'Very High']
        high = [a for a in articles if a.credibility == 'High']
        
        if very_high:
            analysis += "**Quan Ä‘iá»ƒm tá»« cÃ¡c nguá»“n uy tÃ­n cao:**\n"
            for article in very_high[:3]:
                insight = self._extract_insight(article)
                analysis += f"â€¢ **{article.source}**: {insight}\n"
                analysis += f"  ğŸ“ [{article.title[:50]}...]({article.url})\n\n"
        
        if high and len(very_high) < 2:
            analysis += "**PhÃ¢n tÃ­ch bá»• sung:**\n"
            for article in high[:2]:
                insight = self._extract_insight(article)
                analysis += f"â€¢ **{article.source}**: {insight}\n"
                analysis += f"  ğŸ“ [{article.title[:50]}...]({article.url})\n\n"
        
        return analysis.strip()
    
    def _extract_insight(self, article: RSSFeedResult) -> str:
        """Extract key insight from article"""
        if article.summary and len(article.summary) > 30:
            summary = article.summary.replace('\n', ' ').strip()
            return summary[:120] + "..." if len(summary) > 120 else summary
        return f"Äang phÃ¢n tÃ­ch: {article.title[:80]}..."
    
    def _create_references(self, articles: List[RSSFeedResult]) -> str:
        """Táº¡o danh sÃ¡ch tÃ i liá»‡u tham kháº£o"""
        if not articles:
            return "ğŸ“š **TÃ€I LIá»†U THAM KHáº¢O**\n\nKhÃ´ng cÃ³ bÃ i viáº¿t tham kháº£o."
        
        refs = "ğŸ“š **TÃ€I LIá»†U THAM KHáº¢O CHI TIáº¾T**\n\n"
        
        for i, article in enumerate(articles[:8], 1):
            refs += f"**{i}. {article.title}**\n"
            refs += f"   ğŸŒ Nguá»“n: {article.source} ({article.credibility})\n"
            refs += f"   ğŸ“… NgÃ y: {article.published or 'KhÃ´ng xÃ¡c Ä‘á»‹nh'}\n"
            refs += f"   ğŸ”— Link: {article.url}\n"
            refs += f"   ğŸ¯ Äá»™ liÃªn quan: {article.relevance_score}/10\n\n"
        
        return refs.strip()
    
    def _fallback_summary(self, title: str, content: str) -> Dict[str, Any]:
        """Fallback summary when enhanced features fail"""
        bullets = self._create_bullet_summary(title, content)
        
        return {
            'bullet_summary': bullets,
            'domestic_analysis': "ğŸ‡»ğŸ‡³ **PHÃ‚N TÃCH CHUYÃŠN GIA**: Äang cáº­p nháº­t...",
            'international_analysis': "ğŸŒ **PHÃ‚N TÃCH QUá»C Táº¾**: Äang táº£i...",
            'references': "ğŸ“š **TÃ€I LIá»†U THAM KHáº¢O**: Äang tÃ¬m kiáº¿m...",
            'metadata': {
                'status': 'fallback_mode',
                'processing_time': '0.5s'
            }
        }
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Láº¥y metrics tá»« RSS service"""
        return self.rss_service.get_metrics()
    
    async def health_check(self) -> Dict[str, Any]:
        """Health check toÃ n bá»™ system"""
        return await self.rss_service.health_check()
    
    async def close(self):
        """Clean up resources"""
        await self.rss_service.close() 