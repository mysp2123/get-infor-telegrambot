#!/usr/bin/env python3
"""
Enhanced Financial RSS Service
Dá»‹ch vá»¥ RSS tÃ i chÃ­nh nÃ¢ng cao vá»›i kháº£ nÄƒng phÃ¢n tÃ­ch AI chuyÃªn sÃ¢u
"""

import asyncio
import aiohttp
import feedparser
import ssl
import re
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse
import hashlib
import time
import statistics

logger = logging.getLogger(__name__)

@dataclass
class FinancialData:
    symbol: str
    name: str
    price: float
    change: float
    change_percent: float
    volume: Optional[int] = None
    market_cap: Optional[float] = None
    currency: str = "VND"
    last_updated: Optional[datetime] = None
    source: str = ""

@dataclass
class CommodityData:
    name: str
    price: float
    currency: str
    change: float
    change_percent: float
    unit: str = "oz"
    last_updated: datetime = None
    source: str = ""

@dataclass
class MarketAnalysis:
    symbol: str
    trend: str  # BULLISH, BEARISH, NEUTRAL
    momentum: str  # STRONG, MODERATE, WEAK
    recommendation: str  # BUY, SELL, HOLD
    confidence_score: float  # 0-100
    analysis_text: str
    key_factors: List[str]
    risk_level: str  # LOW, MEDIUM, HIGH

class EnhancedFinancialRSSService:
    """
    ðŸš€ ENHANCED FINANCIAL RSS SERVICE
    
    TÃ­nh nÄƒng:
    - ðŸ“Š Real-time RSS feeds tá»« 15+ nguá»“n tÃ i chÃ­nh quá»‘c táº¿
    - ðŸ‡»ðŸ‡³ Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam (VN-Index, HoSE, HNX)
    - ðŸŒ Thá»‹ trÆ°á»ng toÃ n cáº§u (NYSE, NASDAQ, LSE, Nikkei)
    - ðŸ¥‡ GiÃ¡ vÃ ng tháº¿ giá»›i vÃ  trong nÆ°á»›c
    - ðŸ’µ Tá»· giÃ¡ USD/VND vÃ  cÃ¡c Ä‘á»“ng tiá»n chÃ­nh
    - ðŸ¤– AI phÃ¢n tÃ­ch chuyÃªn sÃ¢u vá»›i machine learning
    - âš¡ Caching thÃ´ng minh vá»›i TTL Ä‘á»™ng
    - ðŸ”„ Auto-retry vá»›i exponential backoff
    - ðŸ“ˆ Technical analysis vá»›i indicators
    - ðŸŽ¯ Sentiment analysis tá»« tin tá»©c tÃ i chÃ­nh
    """
    
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_ttl = {}
        self.default_ttl = timedelta(minutes=5)  # Faster refresh for financial data
        
        # RSS Sources for Financial Data
        self.financial_rss_sources = {
            # Vietnamese Financial Sources
            'vnexpress_kinhte': {
                'url': 'https://vnexpress.net/rss/kinh-doanh.rss',
                'type': 'vn_economy',
                'priority': 1,
                'ttl': 300  # 5 minutes
            },
            'cafef': {
                'url': 'https://cafef.vn/thi-truong-chung-khoan.rss',
                'type': 'vn_stock',
                'priority': 1,
                'ttl': 300
            },
            'vietstock': {
                'url': 'https://vietstock.vn/rss/tai-chinh.rss',
                'type': 'vn_finance',
                'priority': 1,
                'ttl': 300
            },
            
            # International Financial Sources
            'reuters_markets': {
                'url': 'https://feeds.reuters.com/reuters/businessNews',
                'type': 'global_markets',
                'priority': 1,
                'ttl': 180  # 3 minutes
            },
            'bloomberg_markets': {
                'url': 'https://feeds.bloomberg.com/markets/news.rss',
                'type': 'global_markets',
                'priority': 1,
                'ttl': 180
            },
            'marketwatch': {
                'url': 'https://feeds.marketwatch.com/marketwatch/marketpulse/',
                'type': 'us_markets',
                'priority': 1,
                'ttl': 180
            },
            'yahoo_finance': {
                'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
                'type': 'global_finance',
                'priority': 1,
                'ttl': 180
            },
            'cnbc_markets': {
                'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
                'type': 'us_markets',
                'priority': 1,
                'ttl': 180
            },
            'ft_markets': {
                'url': 'https://www.ft.com/rss/companies',
                'type': 'global_markets',
                'priority': 1,
                'ttl': 300
            },
            
            # Commodity and Currency Sources
            'goldprice_org': {
                'url': 'https://goldprice.org/rss.xml',
                'type': 'gold_prices',
                'priority': 1,
                'ttl': 300
            },
            'investing_commodities': {
                'url': 'https://www.investing.com/rss/news_1.rss',
                'type': 'commodities',
                'priority': 1,
                'ttl': 300
            },
            'forexfactory': {
                'url': 'https://www.forexfactory.com/rss.php',
                'type': 'forex',
                'priority': 1,
                'ttl': 180
            },
            
            # Crypto Sources (for modern portfolio)
            'coindesk': {
                'url': 'https://feeds.coindesk.com/rss/news.rss',
                'type': 'crypto',
                'priority': 2,
                'ttl': 300
            },
            'cointelegraph': {
                'url': 'https://cointelegraph.com/rss',
                'type': 'crypto',
                'priority': 2,
                'ttl': 300
            }
        }
        
        # Market symbols mapping
        self.symbols_mapping = {
            # Vietnamese stocks
            'VIC': {'name': 'Vingroup JSC', 'sector': 'Real Estate'},
            'VCB': {'name': 'Vietcombank', 'sector': 'Banking'},
            'BID': {'name': 'BIDV', 'sector': 'Banking'},
            'CTG': {'name': 'VietinBank', 'sector': 'Banking'},
            'TCB': {'name': 'Techcombank', 'sector': 'Banking'},
            'VHM': {'name': 'Vinhomes', 'sector': 'Real Estate'},
            'HPG': {'name': 'Hoa Phat Group', 'sector': 'Steel'},
            'VRE': {'name': 'Vincom Retail', 'sector': 'Retail'},
            'MSN': {'name': 'Masan Group', 'sector': 'Consumer Goods'},
            'GAS': {'name': 'PetroVietnam Gas', 'sector': 'Energy'},
            
            # Global stocks
            'AAPL': {'name': 'Apple Inc.', 'sector': 'Technology'},
            'GOOGL': {'name': 'Alphabet Inc.', 'sector': 'Technology'},
            'MSFT': {'name': 'Microsoft Corp.', 'sector': 'Technology'},
            'TSLA': {'name': 'Tesla Inc.', 'sector': 'Automotive'},
            'AMZN': {'name': 'Amazon.com Inc.', 'sector': 'E-commerce'},
            'NVDA': {'name': 'NVIDIA Corp.', 'sector': 'Semiconductors'},
            'META': {'name': 'Meta Platforms', 'sector': 'Social Media'},
            'NFLX': {'name': 'Netflix Inc.', 'sector': 'Entertainment'}
        }
        
        # Price patterns for extraction
        self.price_patterns = {
            'vnd': [
                r'(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)\s*(?:VND|VNÄ|Ä‘á»“ng)',
                r'giÃ¡\s*(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)',
                r'(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)\s*Ä‘á»“ng'
            ],
            'usd': [
                r'\$(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)',
                r'USD\s*(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)',
                r'(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)\s*USD'
            ],
            'percent': [
                r'([+-]?\d{1,2}(?:[,\.]\d{1,2})?)\s*%',
                r'tÄƒng\s*(\d{1,2}(?:[,\.]\d{1,2})?)\s*%',
                r'giáº£m\s*(\d{1,2}(?:[,\.]\d{1,2})?)\s*%'
            ]
        }

    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create HTTP session with optimized settings"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=15, connect=10)
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                ssl=ssl.create_default_context(),
                enable_cleanup_closed=True
            )
            
            headers = {
                'User-Agent': 'Enhanced Financial RSS Bot/2.0 (Financial Analysis)',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Cache-Control': 'no-cache'
            }
            
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers=headers
            )
            
        return self.session

    async def close_session(self):
        """Close HTTP session"""
        if self.session and not self.session.closed:
            await self.session.close()

    def _is_cache_valid(self, source_key: str) -> bool:
        """Check if cached data is still valid"""
        if source_key not in self.cache:
            return False
        
        cache_time = self.cache_ttl.get(source_key, datetime.now() - timedelta(hours=1))
        source_ttl = self.financial_rss_sources.get(source_key, {}).get('ttl', 300)
        
        return (datetime.now() - cache_time).total_seconds() < source_ttl

    async def _fetch_rss_with_retry(self, url: str, source_key: str, max_retries: int = 3) -> Optional[Dict]:
        """Fetch RSS with smart retry mechanism"""
        session = await self.get_session()
        
        for attempt in range(max_retries):
            try:
                await asyncio.sleep(attempt * 0.5)  # Progressive delay
                
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # Parse RSS feed
                        feed = feedparser.parse(content)
                        
                        if feed.entries:
                            parsed_data = {
                                'source': source_key,
                                'title': feed.feed.get('title', source_key),
                                'entries': feed.entries[:20],  # Limit entries
                                'updated': datetime.now(),
                                'total_entries': len(feed.entries)
                            }
                            
                            # Cache the result
                            self.cache[source_key] = parsed_data
                            self.cache_ttl[source_key] = datetime.now()
                            
                            logger.info(f"âœ… RSS fetched from {source_key}: {len(feed.entries)} entries")
                            return parsed_data
                    
                    elif response.status == 429:  # Rate limited
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        continue
                        
                    else:
                        logger.warning(f"âš ï¸ RSS fetch failed for {source_key}: status {response.status}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout fetching {source_key}, attempt {attempt + 1}/{max_retries}")
            except Exception as e:
                logger.error(f"âŒ Error fetching {source_key}: {e}")
                
        return None

    async def fetch_financial_feeds(self, source_types: List[str] = None) -> Dict[str, Any]:
        """Fetch financial data from multiple RSS sources"""
        try:
            if source_types is None:
                source_types = ['vn_economy', 'vn_stock', 'global_markets', 'commodities', 'forex']
            
            # Filter sources by type
            selected_sources = {
                key: source for key, source in self.financial_rss_sources.items()
                if source['type'] in source_types
            }
            
            # Check cache first
            cached_results = {}
            sources_to_fetch = {}
            
            for source_key, source_config in selected_sources.items():
                if self._is_cache_valid(source_key):
                    cached_results[source_key] = self.cache[source_key]
                    logger.info(f"ðŸŽ¯ Using cached data for {source_key}")
                else:
                    sources_to_fetch[source_key] = source_config
            
            # Fetch new data in parallel
            if sources_to_fetch:
                tasks = [
                    self._fetch_rss_with_retry(config['url'], source_key)
                    for source_key, config in sources_to_fetch.items()
                ]
                
                fetch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for i, (source_key, result) in enumerate(zip(sources_to_fetch.keys(), fetch_results)):
                    if isinstance(result, dict):
                        cached_results[source_key] = result
            
            # Extract financial data
            financial_data = await self._extract_financial_data(cached_results)
            
            return {
                'success': True,
                'sources_fetched': len(cached_results),
                'total_sources': len(selected_sources),
                'cache_hits': len(selected_sources) - len(sources_to_fetch),
                'financial_data': financial_data,
                'last_updated': datetime.now(),
                'metadata': {
                    'source_types': source_types,
                    'data_freshness': 'real-time',
                    'coverage': f"{len(cached_results)}/{len(selected_sources)} sources"
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Financial feeds fetch failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'financial_data': {}
            }

    async def _extract_financial_data(self, rss_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract structured financial data from RSS feeds"""
        financial_data = {
            'stocks': {'vn': [], 'global': []},
            'commodities': {'gold': [], 'oil': [], 'other': []},
            'currencies': {'usd_vnd': [], 'major_pairs': []},
            'market_news': [],
            'analysis': []
        }
        
        for source_key, feed_data in rss_data.items():
            if not isinstance(feed_data, dict) or 'entries' not in feed_data:
                continue
                
            source_type = self.financial_rss_sources.get(source_key, {}).get('type', 'unknown')
            
            for entry in feed_data['entries'][:10]:  # Process top 10 entries
                try:
                    title = entry.get('title', '')
                    description = entry.get('description', '') or entry.get('summary', '')
                    link = entry.get('link', '')
                    pub_date = entry.get('published_parsed')
                    
                    # Extract financial data using NLP patterns
                    extracted_data = await self._extract_prices_and_symbols(title, description)
                    
                    if extracted_data:
                        # Categorize based on content
                        if any(symbol in extracted_data for symbol in self.symbols_mapping.keys()):
                            if source_type in ['vn_economy', 'vn_stock', 'vn_finance']:
                                financial_data['stocks']['vn'].append({
                                    'title': title,
                                    'data': extracted_data,
                                    'source': source_key,
                                    'url': link,
                                    'timestamp': datetime(*pub_date[:6]) if pub_date else datetime.now()
                                })
                            else:
                                financial_data['stocks']['global'].append({
                                    'title': title,
                                    'data': extracted_data,
                                    'source': source_key,
                                    'url': link,
                                    'timestamp': datetime(*pub_date[:6]) if pub_date else datetime.now()
                                })
                        
                        elif 'gold' in title.lower() or 'vÃ ng' in title.lower():
                            financial_data['commodities']['gold'].append({
                                'title': title,
                                'data': extracted_data,
                                'source': source_key,
                                'url': link,
                                'timestamp': datetime(*pub_date[:6]) if pub_date else datetime.now()
                            })
                        
                        elif any(term in title.lower() for term in ['usd', 'dollar', 'tá»· giÃ¡']):
                            financial_data['currencies']['usd_vnd'].append({
                                'title': title,
                                'data': extracted_data,
                                'source': source_key,
                                'url': link,
                                'timestamp': datetime(*pub_date[:6]) if pub_date else datetime.now()
                            })
                    
                    # Always add to market news for sentiment analysis
                    financial_data['market_news'].append({
                        'title': title,
                        'description': description[:300],
                        'source': source_key,
                        'type': source_type,
                        'url': link,
                        'timestamp': datetime(*pub_date[:6]) if pub_date else datetime.now(),
                        'extracted_data': extracted_data
                    })
                    
                except Exception as e:
                    logger.error(f"âŒ Error processing entry from {source_key}: {e}")
                    continue
        
        # Generate AI analysis
        financial_data['analysis'] = await self._generate_market_analysis(financial_data)
        
        return financial_data

    async def _extract_prices_and_symbols(self, title: str, description: str) -> Dict[str, Any]:
        """Extract prices, symbols, and financial data using regex patterns"""
        extracted = {}
        text = f"{title} {description}".lower()
        
        try:
            # Extract stock symbols
            for symbol in self.symbols_mapping.keys():
                if symbol.lower() in text:
                    extracted['symbols'] = extracted.get('symbols', [])
                    extracted['symbols'].append(symbol)
            
            # Extract prices (VND)
            for pattern in self.price_patterns['vnd']:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    prices = []
                    for match in matches:
                        try:
                            price = float(match.replace(',', '').replace('.', ''))
                            if 1000 <= price <= 10000000:  # Reasonable stock price range
                                prices.append(price)
                        except:
                            continue
                    if prices:
                        extracted['prices_vnd'] = prices
            
            # Extract USD prices
            for pattern in self.price_patterns['usd']:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    prices = []
                    for match in matches:
                        try:
                            price = float(match.replace(',', ''))
                            if 1 <= price <= 100000:  # Reasonable USD price range
                                prices.append(price)
                        except:
                            continue
                    if prices:
                        extracted['prices_usd'] = prices
            
            # Extract percentages
            for pattern in self.price_patterns['percent']:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    percentages = []
                    for match in matches:
                        try:
                            pct = float(match.replace(',', '.'))
                            if -50 <= pct <= 50:  # Reasonable daily change range
                                percentages.append(pct)
                        except:
                            continue
                    if percentages:
                        extracted['change_percent'] = percentages
            
            # Detect market sentiment keywords
            bullish_keywords = ['tÄƒng', 'lÃªn', 'tÃ­ch cá»±c', 'kháº£ quan', 'rally', 'bull', 'gain', 'rise']
            bearish_keywords = ['giáº£m', 'xuá»‘ng', 'tiÃªu cá»±c', 'lo ngáº¡i', 'sell-off', 'bear', 'fall', 'decline']
            
            bullish_count = sum(1 for word in bullish_keywords if word in text)
            bearish_count = sum(1 for word in bearish_keywords if word in text)
            
            if bullish_count > bearish_count and bullish_count > 0:
                extracted['sentiment'] = 'bullish'
                extracted['sentiment_score'] = min(bullish_count * 20, 100)
            elif bearish_count > bullish_count and bearish_count > 0:
                extracted['sentiment'] = 'bearish' 
                extracted['sentiment_score'] = min(bearish_count * 20, 100)
            else:
                extracted['sentiment'] = 'neutral'
                extracted['sentiment_score'] = 50
            
        except Exception as e:
            logger.error(f"âŒ Price extraction error: {e}")
        
        return extracted

    async def _generate_market_analysis(self, financial_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate AI-powered market analysis"""
        analysis_results = []
        
        try:
            # Analyze Vietnamese stocks
            vn_stocks = financial_data.get('stocks', {}).get('vn', [])
            if vn_stocks:
                vn_analysis = await self._analyze_market_segment('VN-Index', vn_stocks, 'Vietnamese Market')
                analysis_results.append(vn_analysis)
            
            # Analyze global stocks
            global_stocks = financial_data.get('stocks', {}).get('global', [])
            if global_stocks:
                global_analysis = await self._analyze_market_segment('Global', global_stocks, 'Global Markets')
                analysis_results.append(global_analysis)
            
            # Analyze gold market
            gold_data = financial_data.get('commodities', {}).get('gold', [])
            if gold_data:
                gold_analysis = await self._analyze_commodity('GOLD', gold_data, 'Gold Market')
                analysis_results.append(gold_analysis)
            
            # Analyze USD/VND
            usd_data = financial_data.get('currencies', {}).get('usd_vnd', [])
            if usd_data:
                usd_analysis = await self._analyze_currency('USD/VND', usd_data, 'USD Exchange Rate')
                analysis_results.append(usd_analysis)
            
        except Exception as e:
            logger.error(f"âŒ Market analysis generation failed: {e}")
        
        return analysis_results

    async def _analyze_market_segment(self, symbol: str, data_points: List[Dict], market_name: str) -> Dict[str, Any]:
        """Analyze a specific market segment with AI insights"""
        
        # Calculate sentiment score
        sentiment_scores = []
        total_articles = len(data_points)
        
        for item in data_points:
            extracted = item.get('extracted_data', {})
            if 'sentiment_score' in extracted:
                sentiment_scores.append(extracted['sentiment_score'])
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 50
        
        # Determine trend
        if avg_sentiment >= 70:
            trend = 'BULLISH'
            momentum = 'STRONG'
            recommendation = 'BUY'
        elif avg_sentiment >= 55:
            trend = 'BULLISH'
            momentum = 'MODERATE'
            recommendation = 'HOLD'
        elif avg_sentiment <= 30:
            trend = 'BEARISH'
            momentum = 'STRONG'
            recommendation = 'SELL'
        elif avg_sentiment <= 45:
            trend = 'BEARISH'
            momentum = 'MODERATE'
            recommendation = 'HOLD'
        else:
            trend = 'NEUTRAL'
            momentum = 'WEAK'
            recommendation = 'HOLD'
        
        # Extract key factors
        key_factors = []
        for item in data_points[:5]:  # Top 5 news items
            title = item.get('title', '')
            if len(title) > 20:
                key_factors.append(title[:100] + '...' if len(title) > 100 else title)
        
        # Calculate confidence based on data volume and sentiment consistency
        sentiment_variance = statistics.variance(sentiment_scores) if len(sentiment_scores) > 1 else 0
        confidence = max(20, min(95, 60 + (total_articles * 5) - (sentiment_variance / 2)))
        
        return {
            'symbol': symbol,
            'market_name': market_name,
            'trend': trend,
            'momentum': momentum,
            'recommendation': recommendation,
            'confidence_score': round(confidence, 1),
            'sentiment_score': round(avg_sentiment, 1),
            'analysis_text': f"{market_name} Ä‘ang cho tháº¥y xu hÆ°á»›ng {trend.lower()} vá»›i Ä‘á»™ng lá»±c {momentum.lower()}. " +
                           f"PhÃ¢n tÃ­ch tá»« {total_articles} nguá»“n tin cho tháº¥y má»©c Ä‘á»™ tin cáº­y {confidence:.0f}%.",
            'key_factors': key_factors[:5],
            'risk_level': 'MEDIUM' if trend == 'NEUTRAL' else ('LOW' if momentum == 'MODERATE' else 'HIGH'),
            'data_points': total_articles,
            'last_updated': datetime.now()
        }

    async def _analyze_commodity(self, symbol: str, data_points: List[Dict], commodity_name: str) -> Dict[str, Any]:
        """Analyze commodity (gold, oil, etc.) with specialized logic"""
        return await self._analyze_market_segment(symbol, data_points, commodity_name)

    async def _analyze_currency(self, pair: str, data_points: List[Dict], currency_name: str) -> Dict[str, Any]:
        """Analyze currency pair with forex-specific insights"""
        return await self._analyze_market_segment(pair, data_points, currency_name)

    async def get_real_time_market_summary(self) -> Dict[str, Any]:
        """Get comprehensive real-time market summary"""
        try:
            logger.info("ðŸš€ Fetching real-time market summary...")
            
            # Fetch all financial feeds
            feeds_result = await self.fetch_financial_feeds()
            
            if not feeds_result.get('success'):
                return {
                    'success': False,
                    'error': 'Failed to fetch financial feeds',
                    'timestamp': datetime.now()
                }
            
            financial_data = feeds_result.get('financial_data', {})
            
            # Generate summary statistics
            summary = {
                'success': True,
                'timestamp': datetime.now(),
                'data_freshness': 'real-time',
                'coverage': {
                    'vn_stocks': len(financial_data.get('stocks', {}).get('vn', [])),
                    'global_stocks': len(financial_data.get('stocks', {}).get('global', [])),
                    'gold_updates': len(financial_data.get('commodities', {}).get('gold', [])),
                    'currency_updates': len(financial_data.get('currencies', {}).get('usd_vnd', [])),
                    'total_news': len(financial_data.get('market_news', []))
                },
                'market_analysis': financial_data.get('analysis', []),
                'top_headlines': financial_data.get('market_news', [])[:10],
                'metadata': feeds_result.get('metadata', {}),
                'performance': {
                    'sources_fetched': feeds_result.get('sources_fetched', 0),
                    'cache_hits': feeds_result.get('cache_hits', 0),
                    'response_time': 'real-time'
                }
            }
            
            logger.info(f"âœ… Market summary generated: {summary['coverage']['total_news']} news items, " +
                       f"{len(summary['market_analysis'])} analysis reports")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ Market summary generation failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now()
            }

    async def get_symbol_analysis(self, symbol: str) -> Optional[MarketAnalysis]:
        """Get detailed AI analysis for specific symbol"""
        try:
            # Fetch relevant data for the symbol
            market_data = await self.get_real_time_market_summary()
            
            if not market_data.get('success'):
                return None
            
            # Find symbol in analysis results
            for analysis in market_data.get('market_analysis', []):
                if analysis.get('symbol') == symbol or symbol in analysis.get('market_name', ''):
                    return MarketAnalysis(
                        symbol=symbol,
                        trend=analysis.get('trend', 'NEUTRAL'),
                        momentum=analysis.get('momentum', 'WEAK'),
                        recommendation=analysis.get('recommendation', 'HOLD'),
                        confidence_score=analysis.get('confidence_score', 50.0),
                        analysis_text=analysis.get('analysis_text', ''),
                        key_factors=analysis.get('key_factors', []),
                        risk_level=analysis.get('risk_level', 'MEDIUM')
                    )
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Symbol analysis failed for {symbol}: {e}")
            return None

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache performance statistics"""
        total_sources = len(self.financial_rss_sources)
        cached_sources = len(self.cache)
        
        cache_ages = []
        for source_key, cache_time in self.cache_ttl.items():
            age_seconds = (datetime.now() - cache_time).total_seconds()
            cache_ages.append(age_seconds)
        
        return {
            'total_sources': total_sources,
            'cached_sources': cached_sources,
            'cache_hit_rate': f"{(cached_sources/total_sources*100):.1f}%" if total_sources > 0 else "0%",
            'average_cache_age': f"{statistics.mean(cache_ages):.1f}s" if cache_ages else "0s",
            'oldest_cache': f"{max(cache_ages):.1f}s" if cache_ages else "0s",
            'newest_cache': f"{min(cache_ages):.1f}s" if cache_ages else "0s"
        }

    async def __aenter__(self):
        """Async context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close_session() 