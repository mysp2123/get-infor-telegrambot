#!/usr/bin/env python3
"""
Enhanced AI Service with Groq + Gemini Integration
S·ª≠ d·ª•ng Groq l√†m primary provider, Gemini l√†m fallback
"""

import asyncio
import aiohttp
import json
import logging
from typing import List, Dict, Optional
from datetime import datetime
import google.generativeai as genai
from config import Config
from models.article import Article

logger = logging.getLogger(__name__)

class EnhancedAIService:
    """Enhanced AI Service with Groq primary, Gemini fallback"""
    
    def __init__(self):
        self.config = Config()
        
        # Groq configuration
        self.groq_api_keys = [
            "your-groq-key-1-here",
            "your-groq-key-2-here",
            "your-groq-key-3-here"
        ]
        self.current_groq_key_index = 0
        
        # Gemini fallback configuration
        self.gemini_api_keys = self.config.get_active_api_keys('gemini')
        self.current_gemini_key_index = 0
        
        # Groq models prioritized by performance
        self.groq_models = [
            "llama-3.3-70b-versatile",  # Latest and most capable
            "meta-llama/llama-4-scout-17b-16e-instruct",  # NEW Llama 4
            "deepseek-r1-distill-llama-70b",  # Good reasoning
            "llama-3.1-8b-instant",  # Fast for simple tasks
            "gemma2-9b-it"  # Good alternative
        ]
        
        # Initialize Gemini fallback
        self._setup_gemini_fallback()
        
        # Usage stats
        self.usage_stats = {
            'groq_requests': 0,
            'groq_success': 0,
            'gemini_requests': 0,
            'gemini_success': 0,
            'total_requests': 0
        }
        
    def _setup_gemini_fallback(self):
        """Setup Gemini as fallback"""
        if self.gemini_api_keys:
            try:
                genai.configure(api_key=self.gemini_api_keys[self.current_gemini_key_index])
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("‚úÖ Gemini fallback configured successfully")
            except Exception as e:
                logger.error(f"‚ùå Gemini fallback setup failed: {e}")
                self.gemini_model = None
        else:
            self.gemini_model = None
            logger.warning("‚ö†Ô∏è No Gemini API keys found for fallback")
    
    async def _make_groq_request(self, prompt: str, model: str = None, max_tokens: int = 1000, temperature: float = 0.7) -> Dict:
        """Make request to Groq API"""
        if not model:
            model = self.groq_models[0]  # Default to best model
            
        headers = {
            "Authorization": f"Bearer {self.groq_api_keys[self.current_groq_key_index]}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        self.usage_stats['groq_requests'] += 1
        self.usage_stats['total_requests'] += 1
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        content = result['choices'][0]['message']['content']
                        self.usage_stats['groq_success'] += 1
                        logger.info(f"‚úÖ Groq request successful with {model}")
                        return {
                            'success': True,
                            'content': content,
                            'provider': 'groq',
                            'model': model
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Groq API error {response.status}: {error_text}")
                        
                        # Try to rotate key if quota exceeded
                        if response.status == 429:
                            self._rotate_groq_key()
                            
                        return {
                            'success': False,
                            'error': f"Groq API error {response.status}",
                            'provider': 'groq'
                        }
                        
        except asyncio.TimeoutError:
            logger.error("‚è±Ô∏è Groq API timeout")
            return {'success': False, 'error': 'Groq API timeout', 'provider': 'groq'}
        except Exception as e:
            logger.error(f"‚ùå Groq request failed: {e}")
            return {'success': False, 'error': str(e), 'provider': 'groq'}
    
    def _rotate_groq_key(self):
        """Rotate to next Groq API key"""
        self.current_groq_key_index = (self.current_groq_key_index + 1) % len(self.groq_api_keys)
        logger.info(f"üîÑ Rotated to Groq key #{self.current_groq_key_index + 1}")
    
    async def _make_gemini_request(self, prompt: str) -> Dict:
        """Make request to Gemini API as fallback"""
        if not self.gemini_model:
            return {'success': False, 'error': 'Gemini not configured', 'provider': 'gemini'}
        
        self.usage_stats['gemini_requests'] += 1
        
        try:
            response = await asyncio.to_thread(self.gemini_model.generate_content, prompt)
            content = response.text
            self.usage_stats['gemini_success'] += 1
            logger.info("‚úÖ Gemini fallback request successful")
            return {
                'success': True,
                'content': content,
                'provider': 'gemini',
                'model': 'gemini-1.5-flash'
            }
        except Exception as e:
            logger.error(f"‚ùå Gemini fallback failed: {e}")
            return {'success': False, 'error': str(e), 'provider': 'gemini'}
    
    async def generate_content(self, prompt: str, prefer_fast: bool = False) -> str:
        """Generate content with Groq primary, Gemini fallback"""
        
        # Choose model based on task
        if prefer_fast:
            model = "llama-3.1-8b-instant"  # Fast model for simple tasks
        else:
            model = "llama-3.3-70b-versatile"  # Best model for complex tasks
        
        # Try Groq first
        logger.info(f"üöÄ Trying Groq with model: {model}")
        result = await self._make_groq_request(prompt, model)
        
        if result['success']:
            return result['content']
        
        # Try different Groq model if first failed
        for backup_model in self.groq_models[1:]:
            if backup_model != model:
                logger.info(f"üîÑ Trying backup Groq model: {backup_model}")
                result = await self._make_groq_request(prompt, backup_model)
                if result['success']:
                    return result['content']
        
        # Fallback to Gemini
        logger.info("‚ö†Ô∏è Groq failed, falling back to Gemini")
        result = await self._make_gemini_request(prompt)
        
        if result['success']:
            return result['content']
        
        # All failed
        logger.error("‚ùå All AI providers failed")
        return "‚ùå Xin l·ªói, kh√¥ng th·ªÉ t·∫°o n·ªôi dung l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."
    
    async def generate_custom_content(self, prompt: str) -> str:
        """Generate custom content (alias for compatibility)"""
        return await self.generate_content(prompt)
    
    async def generate_article_summary(self, article: Article) -> str:
        """Generate Vietnamese summary for article"""
        prompt = f"""
        T√≥m t·∫Øt b√†i b√°o sau ƒë√¢y b·∫±ng ti·∫øng Vi·ªát, n√™u b·∫≠t t√≠nh li√™n quan v√† s·ª©c h·∫•p d·∫´n:
        
        Ti√™u ƒë·ªÅ: {article.title}
        N·ªôi dung: {article.content[:1500]}...
        Ngu·ªìn: {article.source}
        
        T·∫≠p trung v√†o:
        - C√°c ƒëi·ªÉm ch√≠nh v√† √Ω nghƒ©a
        - T·∫°i sao c√¢u chuy·ªán n√†y quan tr·ªçng
        - C√°c kh√≠a c·∫°nh g√¢y tranh c√£i ho·∫∑c th√∫ v·ªã
        - T√°c ƒë·ªông ƒë·∫øn Vi·ªát Nam (n·∫øu c√≥)
        
        Vi·∫øt ng·∫Øn g·ªçn (100-150 t·ª´), phong c√°ch chuy√™n gia nh∆∞ng d·ªÖ hi·ªÉu.
        """
        
        return await self.generate_content(prompt, prefer_fast=True)
    
    async def generate_facebook_post(self, article: Article, style: str = "expert", expert_posts: List[Dict] = None) -> str:
        """Generate Facebook post with specified style"""
        
        # Style templates
        style_prompts = {
            "expert": "Vi·∫øt v·ªõi gi·ªçng ƒëi·ªáu chuy√™n gia c√≥ uy t√≠n, ph√¢n t√≠ch chuy√™n s√¢u",
            "friendly": "Vi·∫øt th√¢n thi·ªán, g·∫ßn g≈©i nh∆∞ n√≥i chuy·ªán v·ªõi b·∫°n b√®",
            "news": "Vi·∫øt theo phong c√°ch b√°o ch√≠, kh√°ch quan v√† ch√≠nh x√°c",
            "debate": "Vi·∫øt theo phong c√°ch tranh lu·∫≠n, n√™u nhi·ªÅu quan ƒëi·ªÉm",
            "educational": "Vi·∫øt theo phong c√°ch gi√°o d·ª•c, gi·∫£i th√≠ch d·ªÖ hi·ªÉu",
            "inspirational": "Vi·∫øt theo phong c√°ch truy·ªÅn c·∫£m h·ª©ng, t√≠ch c·ª±c"
        }
        
        style_instruction = style_prompts.get(style, style_prompts["expert"])
        
        expert_context = ""
        if expert_posts:
            expert_context = f"""
            
            C√°c b√†i vi·∫øt li√™n quan t·ª´ chuy√™n gia:
            {chr(10).join([post.get('content', '')[:200] + '...' for post in expert_posts[:3]])}
            """
        
        prompt = f"""
        T·∫°o m·ªôt b√†i vi·∫øt Facebook b·∫±ng ti·∫øng Vi·ªát (250-400 t·ª´) d·ª±a tr√™n b√†i b√°o n√†y:
        
        Ti√™u ƒë·ªÅ: {article.title}
        N·ªôi dung: {article.content[:2000]}
        URL: {article.url}
        Ngu·ªìn: {article.source}
        {expert_context}
        
        Phong c√°ch: {style_instruction}
        
        Y√™u c·∫ßu:
        - Th√™m y·∫øu t·ªë h·∫•p d·∫´n v√† d·ªÖ chia s·∫ª
        - Th√™m hashtag ph√π h·ª£p
        - Tham kh·∫£o ngu·ªìn tin
        - Ph√¢n t√≠ch t√°c ƒë·ªông (n·∫øu c√≥)
        - S·ª≠ d·ª•ng emoji ph√π h·ª£p
        - Tr√°nh markdown ph·ª©c t·∫°p
        
        Vi·∫øt t·ª± nhi√™n, kh√¥ng s·ª≠ d·ª•ng k√Ω t·ª± ƒë·∫∑c bi·ªát g√¢y l·ªói.
        """
        
        return await self.generate_content(prompt)
    
    async def generate_image_prompt(self, article: Article, context: str = "") -> str:
        """Generate image prompt based on article content"""
        prompt = f"""
        T·∫°o prompt t·∫°o ·∫£nh AI chi ti·∫øt cho b√†i b√°o n√†y:
        
        Ti√™u ƒë·ªÅ: {article.title}
        N·ªôi dung ch√≠nh: {article.content[:1000]}
        Context: {context}
        
        Y√™u c·∫ßu prompt:
        - M√¥ t·∫£ h√¨nh ·∫£nh c·ª• th·ªÉ, r√µ r√†ng
        - Ph√π h·ª£p v·ªõi n·ªôi dung b√†i b√°o
        - Phong c√°ch chuy√™n nghi·ªáp, tin t·ª©c
        - Th√™m logo PioneerX g√≥c d∆∞·ªõi ph·∫£i
        - M√†u s·∫Øc ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ
        - Kh√¥ng ch·ª©a text trong ·∫£nh
        
        Ch·ªâ tr·∫£ v·ªÅ prompt ti·∫øng Anh, kh√¥ng gi·∫£i th√≠ch th√™m.
        """
        
        return await self.generate_content(prompt, prefer_fast=True)
    
    def get_usage_stats(self) -> Dict:
        """Get current usage statistics"""
        return {
            **self.usage_stats,
            'groq_success_rate': f"{(self.usage_stats['groq_success'] / max(self.usage_stats['groq_requests'], 1) * 100):.1f}%",
            'gemini_success_rate': f"{(self.usage_stats['gemini_success'] / max(self.usage_stats['gemini_requests'], 1) * 100):.1f}%",
            'current_groq_key': self.current_groq_key_index + 1,
            'available_groq_keys': len(self.groq_api_keys),
            'gemini_fallback_available': self.gemini_model is not None
        }
    
    def get_api_status(self) -> dict:
        """Get API status for compatibility"""
        return self.get_usage_stats() 