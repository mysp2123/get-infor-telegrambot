#!/usr/bin/env python3
"""
Enhanced AI Service with Groq + Gemini Integration
Sá»­ dá»¥ng Groq lÃ m primary provider, Gemini lÃ m fallback
"""

import asyncio
import aiohttp
import json
import logging
from typing import List, Dict, Optional
from datetime import datetime
import google.generativeai as genai
from config import Config
from models.article import Article

logger = logging.getLogger(__name__)

class EnhancedAIService:
    """Enhanced AI Service with Groq primary, Gemini fallback"""
    
    def __init__(self):
        self.config = Config()
        
        # Groq configuration
        self.groq_api_keys = [
            "your-groq-key-1-here",
            "your-groq-key-2-here",
            "your-groq-key-3-here"
        ]
        self.current_groq_key_index = 0
        
        # Gemini fallback configuration
        self.gemini_api_keys = self.config.get_active_api_keys('gemini')
        self.current_gemini_key_index = 0
        
        # Groq models prioritized by performance
        self.groq_models = [
            "llama-3.3-70b-versatile",  # Latest and most capable
            "meta-llama/llama-4-scout-17b-16e-instruct",  # NEW Llama 4
            "deepseek-r1-distill-llama-70b",  # Good reasoning
            "llama-3.1-8b-instant",  # Fast for simple tasks
            "gemma2-9b-it"  # Good alternative
        ]
        
        # Initialize Gemini fallback
        self._setup_gemini_fallback()
        
        # Usage stats
        self.usage_stats = {
            'groq_requests': 0,
            'groq_success': 0,
            'gemini_requests': 0,
            'gemini_success': 0,
            'total_requests': 0
        }
        
    def _setup_gemini_fallback(self):
        """Setup Gemini as fallback"""
        if self.gemini_api_keys:
            try:
                genai.configure(api_key=self.gemini_api_keys[self.current_gemini_key_index])
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("âœ… Gemini fallback configured successfully")
            except Exception as e:
                logger.error(f"âŒ Gemini fallback setup failed: {e}")
                self.gemini_model = None
        else:
            self.gemini_model = None
            logger.warning("âš ï¸ No Gemini API keys found for fallback")
    
    async def _make_groq_request(self, prompt: str, model: str = None, max_tokens: int = 1000, temperature: float = 0.7) -> Dict:
        """Make request to Groq API"""
        if not model:
            model = self.groq_models[0]  # Default to best model
            
        headers = {
            "Authorization": f"Bearer {self.groq_api_keys[self.current_groq_key_index]}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        self.usage_stats['groq_requests'] += 1
        self.usage_stats['total_requests'] += 1
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        content = result['choices'][0]['message']['content']
                        self.usage_stats['groq_success'] += 1
                        logger.info(f"âœ… Groq request successful with {model}")
                        return {
                            'success': True,
                            'content': content,
                            'provider': 'groq',
                            'model': model
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Groq API error {response.status}: {error_text}")
                        
                        # Try to rotate key if quota exceeded
                        if response.status == 429:
                            self._rotate_groq_key()
                            
                        return {
                            'success': False,
                            'error': f"Groq API error {response.status}",
                            'provider': 'groq'
                        }
                        
        except asyncio.TimeoutError:
            logger.error("â±ï¸ Groq API timeout")
            return {'success': False, 'error': 'Groq API timeout', 'provider': 'groq'}
        except Exception as e:
            logger.error(f"âŒ Groq request failed: {e}")
            return {'success': False, 'error': str(e), 'provider': 'groq'}
    
    def _rotate_groq_key(self):
        """Rotate to next Groq API key"""
        self.current_groq_key_index = (self.current_groq_key_index + 1) % len(self.groq_api_keys)
        logger.info(f"ğŸ”„ Rotated to Groq key #{self.current_groq_key_index + 1}")
    
    async def _make_gemini_request(self, prompt: str) -> Dict:
        """Make request to Gemini API as fallback"""
        if not self.gemini_model:
            return {'success': False, 'error': 'Gemini not configured', 'provider': 'gemini'}
        
        self.usage_stats['gemini_requests'] += 1
        
        try:
            response = await asyncio.to_thread(self.gemini_model.generate_content, prompt)
            content = response.text
            self.usage_stats['gemini_success'] += 1
            logger.info("âœ… Gemini fallback request successful")
            return {
                'success': True,
                'content': content,
                'provider': 'gemini',
                'model': 'gemini-1.5-flash'
            }
        except Exception as e:
            logger.error(f"âŒ Gemini fallback failed: {e}")
            return {'success': False, 'error': str(e), 'provider': 'gemini'}
    
    async def generate_content(self, prompt: str, prefer_fast: bool = False) -> str:
        """Generate content with Groq primary, Gemini fallback"""
        
        # Choose model based on task
        if prefer_fast:
            model = "llama-3.1-8b-instant"  # Fast model for simple tasks
        else:
            model = "llama-3.3-70b-versatile"  # Best model for complex tasks
        
        # Try Groq first
        logger.info(f"ğŸš€ Trying Groq with model: {model}")
        result = await self._make_groq_request(prompt, model)
        
        if result['success']:
            return result['content']
        
        # Try different Groq model if first failed
        for backup_model in self.groq_models[1:]:
            if backup_model != model:
                logger.info(f"ğŸ”„ Trying backup Groq model: {backup_model}")
                result = await self._make_groq_request(prompt, backup_model)
                if result['success']:
                    return result['content']
        
        # Fallback to Gemini
        logger.info("âš ï¸ Groq failed, falling back to Gemini")
        result = await self._make_gemini_request(prompt)
        
        if result['success']:
            return result['content']
        
        # All failed
        logger.error("âŒ All AI providers failed")
        return "âŒ Xin lá»—i, khÃ´ng thá»ƒ táº¡o ná»™i dung lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau."
    
    async def generate_custom_content(self, prompt: str) -> str:
        """Generate custom content (alias for compatibility)"""
        return await self.generate_content(prompt)
    
    async def generate_article_summary(self, article: Article) -> str:
        """Generate Vietnamese summary for article"""
        prompt = f"""
        TÃ³m táº¯t bÃ i bÃ¡o sau Ä‘Ã¢y báº±ng tiáº¿ng Viá»‡t, nÃªu báº­t tÃ­nh liÃªn quan vÃ  sá»©c háº¥p dáº«n:
        
        TiÃªu Ä‘á»: {article.title}
        Ná»™i dung: {article.content[:1500]}...
        Nguá»“n: {article.source}
        
        Táº­p trung vÃ o:
        - CÃ¡c Ä‘iá»ƒm chÃ­nh vÃ  Ã½ nghÄ©a
        - Táº¡i sao cÃ¢u chuyá»‡n nÃ y quan trá»ng
        - CÃ¡c khÃ­a cáº¡nh gÃ¢y tranh cÃ£i hoáº·c thÃº vá»‹
        - TÃ¡c Ä‘á»™ng Ä‘áº¿n Viá»‡t Nam (náº¿u cÃ³)
        
        Viáº¿t ngáº¯n gá»n (100-150 tá»«), phong cÃ¡ch chuyÃªn gia nhÆ°ng dá»… hiá»ƒu.
        """
        
        return await self.generate_content(prompt, prefer_fast=True)
    
    async def generate_facebook_post(self, article: Article, style: str = "expert", expert_posts: List[Dict] = None) -> str:
        """Generate Facebook post with specified style"""
        
        # Style templates
        style_prompts = {
            "expert": "Viáº¿t vá»›i giá»ng Ä‘iá»‡u chuyÃªn gia cÃ³ uy tÃ­n, phÃ¢n tÃ­ch chuyÃªn sÃ¢u",
            "friendly": "Viáº¿t thÃ¢n thiá»‡n, gáº§n gÅ©i nhÆ° nÃ³i chuyá»‡n vá»›i báº¡n bÃ¨",
            "news": "Viáº¿t theo phong cÃ¡ch bÃ¡o chÃ­, khÃ¡ch quan vÃ  chÃ­nh xÃ¡c",
            "debate": "Viáº¿t theo phong cÃ¡ch tranh luáº­n, nÃªu nhiá»u quan Ä‘iá»ƒm",
            "educational": "Viáº¿t theo phong cÃ¡ch giÃ¡o dá»¥c, giáº£i thÃ­ch dá»… hiá»ƒu",
            "inspirational": "Viáº¿t theo phong cÃ¡ch truyá»n cáº£m há»©ng, tÃ­ch cá»±c"
        }
        
        style_instruction = style_prompts.get(style, style_prompts["expert"])
        
        expert_context = ""
        if expert_posts:
            expert_context = f"""
            
            CÃ¡c bÃ i viáº¿t liÃªn quan tá»« chuyÃªn gia:
            {chr(10).join([post.get('content', '')[:200] + '...' for post in expert_posts[:3]])}
            """
        
        prompt = f"""
        Táº¡o má»™t bÃ i viáº¿t Facebook báº±ng tiáº¿ng Viá»‡t (250-400 tá»«) dá»±a trÃªn bÃ i bÃ¡o nÃ y:
        
        TiÃªu Ä‘á»: {article.title}
        Ná»™i dung: {article.content[:2000]}
        URL: {article.url}
        Nguá»“n: {article.source}
        {expert_context}
        
        Phong cÃ¡ch: {style_instruction}
        
        YÃªu cáº§u:
        - ThÃªm yáº¿u tá»‘ háº¥p dáº«n vÃ  dá»… chia sáº»
        - ThÃªm hashtag phÃ¹ há»£p
        - Tham kháº£o nguá»“n tin
        - PhÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng (náº¿u cÃ³)
        - Sá»­ dá»¥ng emoji phÃ¹ há»£p
        - TrÃ¡nh markdown phá»©c táº¡p
        
        Viáº¿t tá»± nhiÃªn, khÃ´ng sá»­ dá»¥ng kÃ½ tá»± Ä‘áº·c biá»‡t gÃ¢y lá»—i.
        """
        
        return await self.generate_content(prompt)
    
    async def generate_image_prompt(self, article: Article, context: str = "") -> str:
        """Generate image prompt based on article content"""
        prompt = f"""
        Táº¡o prompt táº¡o áº£nh AI chi tiáº¿t cho bÃ i bÃ¡o nÃ y:
        
        TiÃªu Ä‘á»: {article.title}
        Ná»™i dung chÃ­nh: {article.content[:1000]}
        Context: {context}
        
        YÃªu cáº§u prompt:
        - MÃ´ táº£ hÃ¬nh áº£nh cá»¥ thá»ƒ, rÃµ rÃ ng
        - PhÃ¹ há»£p vá»›i ná»™i dung bÃ i bÃ¡o
        - Phong cÃ¡ch chuyÃªn nghiá»‡p, tin tá»©c
        - ThÃªm logo PioneerX gÃ³c dÆ°á»›i pháº£i
        - MÃ u sáº¯c phÃ¹ há»£p vá»›i chá»§ Ä‘á»
        - KhÃ´ng chá»©a text trong áº£nh
        
        Chá»‰ tráº£ vá» prompt tiáº¿ng Anh, khÃ´ng giáº£i thÃ­ch thÃªm.
        """
        
        return await self.generate_content(prompt, prefer_fast=True)
    
    def get_usage_stats(self) -> Dict:
        """Get current usage statistics"""
        return {
            **self.usage_stats,
            'groq_success_rate': f"{(self.usage_stats['groq_success'] / max(self.usage_stats['groq_requests'], 1) * 100):.1f}%",
            'gemini_success_rate': f"{(self.usage_stats['gemini_success'] / max(self.usage_stats['gemini_requests'], 1) * 100):.1f}%",
            'current_groq_key': self.current_groq_key_index + 1,
            'available_groq_keys': len(self.groq_api_keys),
            'gemini_fallback_available': self.gemini_model is not None
        }
    
    def get_api_status(self) -> dict:
        """Get API status for compatibility"""
        return self.get_usage_stats() 